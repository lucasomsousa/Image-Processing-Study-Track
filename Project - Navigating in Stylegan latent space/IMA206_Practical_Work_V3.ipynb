{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMA206_Practical_Work_V3.ipynb","private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch\n","\n","**Notes**\n","This is based on Derrick Schultz's [SG2-ADA-PyTorch notebook](https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"Vj4PG4_i9Alt"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"qGEXPcFJ9UTY"},"source":["Let’s start by checking to see what GPU we’ve been assigned."]},{"cell_type":"code","metadata":{"id":"7VVICTCvd4mc"},"source":["!nvidia-smi -L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSV_HEoD9dxo"},"source":["Next let’s connect our Google Drive account."]},{"cell_type":"code","metadata":{"id":"IuVPuJmbigRs"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTjVmfSK9CYa"},"source":["## Install repo\n","\n","The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."]},{"cell_type":"code","metadata":{"id":"B8ADVNpBh8Ox"},"source":["import os\n","!pip install gdown --upgrade\n","\n","if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n","    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","elif os.path.isdir(\"/content/drive/\"):\n","    #install script\n","    %cd \"/content/drive/MyDrive/\"\n","    !mkdir colab-sg2-ada-pytorch\n","    %cd colab-sg2-ada-pytorch\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n","else:\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    %cd pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n","    %cd ../\n","\n","!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZkcJ58P97Ls"},"source":["## Dataset Preparation\n","\n","Upload a .zip of square images to the `datasets` folder."]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["## Train model"]},{"cell_type":"markdown","metadata":{"id":"bNc-3wTO-MUd"},"source":["Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n","\n","* `dataset_path`: this is the path to your .zip file\n","* `resume_from`: we've uploaded the \"metfaces\" file for network parameters.\n","* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."]},{"cell_type":"code","metadata":{"id":"JV0W6yxP-UIn"},"source":["resume_from = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl'\n","aug_strength = 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl\n","!mv celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl /content/\n"],"metadata":{"id":"JgpdHaFn6VYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/\n","!git clone https://github.com/gtamba/pytorch-slim-cnn "],"metadata":{"id":"anxXXgMBPLsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd pytorch-slim-cnn/"],"metadata":{"id":"CFvnwyd6UKoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from slimnet import SlimNet"],"metadata":{"id":"Y346hFBfSGb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch"],"metadata":{"id":"8oIqg9FRUuBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pickle\n","import PIL\n","import numpy as np\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import random"],"metadata":{"id":"gJDgXiOb7Bu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda')\n","model = SlimNet.load_pretrained('/content/pytorch-slim-cnn/models/celeba_20.pth').to(device)"],"metadata":{"id":"o6K6eiZSU47I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = np.array(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n","       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n","       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n","       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n","       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n","       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n","       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n","       'Wearing_Necklace', 'Wearing_Necktie', 'Young'])"],"metadata":{"id":"oLUuWjMQbMqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","                              transforms.Resize((178,218)),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ])"],"metadata":{"id":"2eYtGTPCZ5wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl', 'rb') as f:\n","    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module"],"metadata":{"id":"LHX0A3hv6cDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get images with/without glasses\n","\n","This was executed once to identify multiple images with and without the presence of glasses. Then, we handpicked the good ones (since some of the images were rather badly generated) and saved these images and the points in latent space used to generate them into .npy files, so we don't have to run the code and handpick the images again every time."],"metadata":{"id":"w7xiFWLIj85m"}},{"cell_type":"code","source":["def generate_img(G,w):\n","  # G is a Generator and w is the point in the mapping space (as a tensor)\n","  img = G.synthesis(w, noise_mode='const', force_fp32=True)\n","  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","  img = img.cpu().numpy()\n","  return img\n","\n","def img_to_tensor(img):\n","  return transform(PIL.Image.fromarray(img, 'RGB')).unsqueeze(0).to(device)"],"metadata":{"id":"P3dTl5K1aouK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# n_imgs = 1\n","# z = torch.randn([n_imgs, G.z_dim]).cuda()    # latent codes\n","# c = None                                # class labels (not used in this example)]\n","# w = G.mapping(z, c, truncation_psi=0.3, truncation_cutoff=8)\n","# img = G.synthesis(w, noise_mode='const', force_fp32=True)"],"metadata":{"id":"oHeC-ZjR-_ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","# x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)"],"metadata":{"id":"nr-jj5NZaOZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PIL.Image.fromarray(img.cpu().numpy(), 'RGB')"],"metadata":{"id":"rkPSYB_jbRTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_female = []\n","z_female = []\n","for cont in range(200):\n","  #target = set([''])\n","  target = set(['Male'])\n","  while 'Male' in target:\n","    n_imgs = 1\n","    z = torch.randn([n_imgs, G.z_dim]).cuda()    # latent codes\n","    c = None                                # class labels (not used in this example)]\n","    w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n","    img = G.synthesis(w, noise_mode='const', force_fp32=True)\n","\n","    img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","      model.eval()\n","      logits = model(x)\n","      sigmoid_logits = torch.sigmoid(logits)\n","      predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n","    target = set(labels[predictions.astype(bool)])\n","  w_female.append(w)\n","  z_female.append(z)"],"metadata":{"id":"lhKlFR7Ea-ED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(z_male)"],"metadata":{"id":"x7-29iGqnmgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z_no_glasses_list = np.array([z.cpu().numpy() for z in z_no_glasses])\n","np.save('/content/drive/MyDrive/IMA206-Project/z_no_glasses_list.npy', np.array(z_no_glasses_list))"],"metadata":{"id":"vMSkV67TmvQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_path = \"/content/drive/MyDrive/IMA206-Project/Female/\"\n","import imageio\n","for i in range(200):\n","  z = z_female[i]\n","  w = w_female[i]\n","  img = generate_img(G, w)\n","  imageio.imwrite(drive_path + f'female_{i:03d}.jpg', img)"],"metadata":{"id":"O3-dXxOOZ7wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_noglasses = []\n","for cont in range(400):\n","  target = set(['Eyeglasses'])\n","  while 'Eyeglasses' in target:\n","    n_imgs = 1\n","    z = torch.randn([n_imgs, G.z_dim]).cuda()    # latent codes\n","    c = None                                # class labels (not used in this example)]\n","    w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n","    img = G.synthesis(w, noise_mode='const', force_fp32=True)\n","\n","    img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","      model.eval()\n","      logits = model(x)\n","      sigmoid_logits = torch.sigmoid(logits)\n","      predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n","    target = set(labels[predictions.astype(bool)])\n","  w_noglasses.append(w)"],"metadata":{"id":"hlgy9KpJjnFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(400):\n","  img = G.synthesis(w_noglasses[i], noise_mode='const', force_fp32=True)\n","  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","  img_print = np.array(PIL .Image.fromarray(img.cpu().numpy(), 'RGB'))\n","  plt.ion()\n","  plt.figure()\n","  plt.imshow(img_print)\n","  plt.title(f'{i}')\n","  plt.show()\n","  _ = input('')\n","  plt.close()"],"metadata":{"id":"Adq3j6AG2qSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["male_index = [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 23, 27, 30, 31, 34, 38, 39, 40, 42, 44, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61, 63, 65, 67, 68, 69, 74, 76, 78, 79, 82, 83, 84, 85, 86, 88, 93, 94, 96, 103, 108, 109, 113]\n","female_index = [3, 6, 7, 8, 11, 12, 13, 16, 18, 20, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 38, 39, 41, 42, 43, 44, 45, 49, 50, 51, 58, 68, 71, 74, 76, 83, 84, 91, 92, 93, 95, 97, 98, 101, 104, 106, 107, 108, 109, 110, 112, 116, 119, 121, 122, 123, 124, 126, 127, 128]\n","\n","young_index = [4, 9, 12, 14, 16, 17, 21, 22, 25, 37, 40, 50, 52, 56, 57, 61, 63, 71, 73, 75, 76, 77, 78, 80, 81, 82, 86, 88, 92, 94, 101, 113, 115, 118, 120, 121, 129, 131, 140, 144, 150, 152, 153, 160, 161, 166, 167, 168, 170, 171, 172, 173, 174, 178, 180, 181, 189, 193, 194, 195]\n","old_index = [4, 7, 9, 10, 11, 18, 19, 21, 22, 27, 31, 32, 36, 38, 40, 42, 45, 47, 49, 50, 52, 54, 56, 57, 59, 62, 64, 65, 68, 69, 74, 75, 79, 87, 95, 101, 106, 109, 111, 114, 117, 119, 120, 121, 125, 130, 131, 133, 134, 138, 139, 141, 150, 151, 153, 155, 164, 167, 176, 178]\n","\n","no_glasses_index = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 19, 23, 24, 25, 27, 28, 29, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 50, 51, 52, 53, 54, 55, 57, 61, 64, 65, 66, 70, 73, 74, 77, 81, 82, 87, 90, 92, 95, 100, 103]\n","glasses_index = [1, 6, 7, 8, 14, 16, 17, 18, 20, 26, 29, 30, 33, 34, 37, 38, 40, 43, 47, 49, 50, 58, 59, 60, 64, 58, 71, 74, 75, 76, 77, 80, 85, 86, 90, 93, 95, 97, 102, 105, 107, 113, 114, 115, 120, 125, 128, 129, 130, 131, 147, 153, 154, 155, 156, 157, 159, 165, 171, 176]\n"],"metadata":{"id":"TTv27nr22-o2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z_no_glasses_list = np.array([z.cpu().numpy() for z in z_no_glasses])\n","w_no_glasses_list = np.array([w.cpu().numpy() for w in w_no_glasses])\n","\n","z_no_glasses_selected = z_no_glasses_list[no_glasses_index]\n","w_no_glasses_selected = w_no_glasses_list[no_glasses_index]\n","\n","np.save('/content/drive/MyDrive/IMA206-Project/w_no_glasses_selected', np.array(w_no_glasses_selected))\n","np.save('/content/drive/MyDrive/IMA206-Project/z_no_glasses_selected', np.array(z_no_glasses_selected))"],"metadata":{"id":"gqtCNDs5rEff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_male_list = np.array([x.cpu().numpy() for x in w_male])\n","w_male_selected = w_male_list[male_index]\n","\n","np.save('/content/w_male_list.npy', np.array(w_male_list))\n","np.save('/content/w_male_selected.npy', np.array(w_male_selected))"],"metadata":{"id":"dryeijn4c9sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_glasses_list = np.array([x.cpu().numpy() for x in w_glasses])\n","w_glasses_list"],"metadata":{"id":"_rGtqUuB63MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_glasses_selected = w_glasses_list[glasses_index]"],"metadata":{"id":"eNDs0egx76l-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_noglasses_list = np.array([x.cpu().numpy() for x in w_noglasses])\n","w_noglasses_list"],"metadata":{"id":"YY3gj3ci8yx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_noglasses_selected = w_noglasses_list[noglasses_index]"],"metadata":{"id":"kA3fMjyh8_zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('/content/w_glasses_list.npy', np.array(w_glasses_list))\n","np.save('/content/w_noglasses_list.npy', np.array(w_noglasses_list))\n","np.save('/content/w_glasses_selected.npy', np.array(w_glasses_selected))\n","np.save('/content/w_noglasses_selected.npy', np.array(w_noglasses_selected))"],"metadata":{"id":"qpI6V_1A-fjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits = model(x)\n","sigmoid_logits = torch.sigmoid(logits)\n","predictions = (sigmoid_logits > 0.5).squeeze().numpy()\n","\n","print(labels[predictions.astype(bool)])"],"metadata":{"id":"9_dBtCfGcN7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_testing = G.synthesis(w[35].unsqueeze(0), noise_mode='const', force_fp32=True)\n","img_testing = (img_testing.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","img_testing = np.array(PIL.Image.fromarray(img_testing.cpu().numpy(), 'RGB'))\n","plt.figure()\n","imshow(img_testing)"],"metadata":{"id":"09A2l-ysGBza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sunglasses.append(w[11])\n","glasses.append(w[34])\n","hat.append(w[36])"],"metadata":{"id":"_7TaCZO1IkSu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load saved data (see above)"],"metadata":{"id":"vlrjc_ickr5l"}},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"TS56E282lWOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown --id 1FFC0tS5YtktEnEA0hTC8Qbqvd1TVVBpR -O /content/w_noglasses_selected.npy\n","!gdown --id 1FAZQGqlTPZHcxCJ3X4u2E3mZVCPlZ90_ -O /content/w_noglasses_list.npy\n","!gdown --id 1OoggWJ0OBXtL0WzskSbU-ufNNqYIAjjY -O /content/w_glasses_selected.npy\n","!gdown --id 1n0_gixUalPr784s7UBS0CgU78R4Iz7BM -O /content/w_glasses_list.npy"],"metadata":{"id":"gDRJ1QmNkxrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w_noglasses_selected = torch.from_numpy(np.load(\"/content/w_noglasses_selected.npy\"))\n","w_noglasses_list = torch.from_numpy(np.load(\"/content/w_noglasses_list.npy\"))\n","w_glasses_selected = torch.from_numpy(np.load(\"/content/w_glasses_selected.npy\"))\n","w_glasses_list = torch.from_numpy(np.load(\"/content/w_glasses_list.npy\"))\n","\n","w_noglasses_selected = w_noglasses_selected.to(device)\n","w_noglasses_list = w_noglasses_list.to(device)\n","w_glasses_selected = w_glasses_selected.to(device)\n","w_glasses_list = w_glasses_list.to(device)"],"metadata":{"id":"5cqIwuhCmY4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_glasses = [generate_img(G,w) for w in w_glasses_selected]\n","img_noglasses = [generate_img(G,w) for w in w_noglasses_selected]\n","\n","# Transform tensors to numpy\n","w_noglasses_arr = np.squeeze(w_noglasses_selected.cpu().numpy())\n","w_glasses_arr = np.squeeze(w_glasses_selected.cpu().numpy())\n","\n","# Space shape\n","space_shape = w_noglasses_arr[0].shape"],"metadata":{"id":"FiEFHL8LrV7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create X (points in mapping space) and y (classes)\n","X = np.concatenate((w_noglasses_arr, w_glasses_arr))\n","y = np.concatenate((np.zeros(w_noglasses_arr.shape[0]), np.ones(w_glasses_arr.shape[0])))\n","\n","# Sort the samples\n","idx = random.sample(list(range(len(y))), len(y))\n","X = X[idx].reshape(X.shape[0], -1)\n","y = y[idx]"],"metadata":{"id":"cO6ePbUeFYQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting separating hyperplane using SVM"],"metadata":{"id":"5iGKhOV8wH-5"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"KUQc48gRwQjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf_pipe = Pipeline([('scaler', StandardScaler()),\n","                          ('clf', SVC(gamma='auto', kernel='linear'))])\n","clf_pipe.fit(X, y)\n"],"metadata":{"id":"UuvCtXXDK4u9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hyperplane = clf_pipe['clf'].coef_[0]\n","hyperplane = hyperplane.reshape(space_shape)\n","hyperplane = torch.from_numpy(hyperplane).to(device)"],"metadata":{"id":"Pr0j-b9418UR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n","import imageio\n","images = []\n","img_n = 35\n","for i in range(500):\n","  w = w_noglasses_selected[img_n] + hyperplane*i\n","  img = generate_img(G, w)\n","  images.append(img)\n","imageio.mimsave(drive_path + f'moving_{img_n}.gif', images, duration=0.02)"],"metadata":{"id":"a9mt5_CS5Jhm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6EtrPqL9ILk"},"source":["## Testing/Inference\n","\n","Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."]},{"cell_type":"markdown","metadata":{"id":"mYdyfH0O8In_"},"source":["### Generate Single Images\n","\n","`--network`: Make sure the `--network` argument points to the .pkl file.\n","\n","`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n","\n","`--truncation`: Truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n"]},{"cell_type":"code","metadata":{"id":"VYRXenMoZSHf"},"source":["!python generate.py --outdir=/content/out/images/ --trunc=1 --seeds=85,265,297,84 --network=$resume_from"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /content/out/images/"],"metadata":{"id":"Y3R8X0MKRovh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import skimage\n","import glob\n","import matplotlib.pyplot as plt\n","from skimage.io import imshow\n","\n","img_names = glob.glob(\"/content/out/images/*\")\n","\n","for fn in img_names:\n","  plt.figure()\n","  imshow(fn)"],"metadata":{"id":"-3edgvimOOBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm /content/out/images/seed*.png"],"metadata":{"id":"mwp-Ic5jRfxD"},"execution_count":null,"outputs":[]}]}